<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Raj Ghugare</title>
    <link rel="stylesheet" href="stylesheet.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
</head>

<body>
    <div class="container">

        <header class="profile-section">

            <div class="profile-image">
                <img src="images/raj.jpg" alt="Raj Ghugare" />
                <p style="color: gray;">rg9360@princeton.edu</p>
                <nav>
                <a href="https://github.com/RajGhugare19">Github</a>|&nbsp;&nbsp;<a href="https://twitter.com/GhugareRaj"> Twitter</a>
                </nav>
            </div>
            <div class="profile-info">
                <h1>Raj Ghugare</h1>
                <p>I am a PhD student at <a href="https://www.princeton.edu/">Princeton University</a>, advised by <a href="https://ben-eysenbach.github.io/">Ben Eysenbach</a>. 
                    Previously, I spent 1.5 years at <a href="https://mila.quebec/en">Mila</a> and <a href="https://montrealrobotics.ca/">Montreal robotics and AI lab</a>. 
                    Before that, I completed my bachelors from <a href="https://en.wikipedia.org/wiki/Visvesvaraya_National_Institute_of_Technology_Nagpur">NIT Nagpur</a>.
                    Broadly, my research goal is to develop simpler and scalable AI algorithms. I am interested in topics revolving reinforcement learning and probabilistic inference.
            </div>
        </header>
        
        <main>
            <section class="research">
                <h2>Research</h2>
                <p>Please refer to <a href="https://scholar.google.com/citations?user=hzxdkrIAAAAJ&hl=en&authuser=1">Google Scholar</a> for a complete list of my publications.</p>
                
                <article class="research-item">
                    <div class="research-image">
                        <img src="images/builder-bench.png" alt="builder-bench" />
                    </div>
                    <div class="research-content">
                        <papertitle>BuilderBench &ndash; A benchmark for generalist agents</papertitle> <em style="color:#696969;">[Preprint]</em> <br> 
                        <authors>
                            <b>Raj Ghugare</b>,
                            Catherine Ji,
                            Kathryn Wantlin,
                            Jin Schofield,
                            Benjamin Eysenbach
                        </authors>
                        <p>
                            <a href="builderbench/index.html">project page</a> |
                            <a href="https://arxiv.org/pdf/2510.06288">paper</a> |
                            <a href="https://github.com/RajGhugare19/builderbench">code</a>
                        </p>
                        <hr style="height:1px;border:none;color:#e0e0e0;background-color:#e0e0e0;">
                        <abstract>
                            We introduce a new benchmark focusing on the evaluation of open-ended exploration and embodied reasoning using block building.
                        </abstract>
                    </div>
                </article>
                
                <article class="research-item">
                    <div class="research-image">
                        <img src="images/voc.png" alt="value-of-compute" />
                    </div>
                    <div class="research-content">
                        <papertitle>On the Role of Iterative Computation in Reinforcement Learning</papertitle> <em style="color:#696969;">[Preprint]</em> <br> 
                        <authors>
                            <b>Raj Ghugare</b>,
                            Micha≈Ç Bortkiewicz*,
                            Alicja Ziarko*,
                            Benjamin Eysenbach
                        </authors>
                        <p>
                            <a href="computation-rl/index.html">project page</a> |
                            <a href="https://arxiv.org/abs/2602.05999">paper</a> |
                            <a href="todo">code</a>
                        </p>
                        <hr style="height:1px;border:none;color:#e0e0e0;background-color:#e0e0e0;">
                        <abstract>
                            In this work we advocate for a computational view of RL,
                            where computation time and parameter count are distinct
                            axes. We theoretically and empirically study how computation
                            impacts policy performance and generalization in RL.
                        </abstract>
                    </div>
                </article>
                
                <article class="research-item">
                    <div class="research-image">
                        <img src="images/nf_block.png" alt="nf capabilities" />
                    </div>
                    <div class="research-content">
                        <papertitle>Normalizing Flows are Capable Models for RL</papertitle> <em style="color:#696969;">[NeurIPS 2025]</em> <br> 
                        <authors>
                            <b>Raj Ghugare</b>,
                            Benjamin Eysenbach
                        </authors>
                        <p>
                            <a href="nf4rl/index.html">project page</a> |
                            <a href="https://arxiv.org/abs/2505.23527">paper</a> |
                            <a href="https://github.com/Princeton-RL/normalising-flows-4-reinforcement-learning">code</a>
                        </p>
                        <hr style="height:1px;border:none;color:#e0e0e0;background-color:#e0e0e0;">
                        <abstract>
                            Normalizing Flows are among the most flexible probabilistic models, yet they have received far less attention from the RL
                            community. We take a step towards correcting this by showing that NFs can indeed be a powerful model for RL.                            
                        </abstract>
                    </div>
                </article>

                <article class="research-item">
                    <div class="research-image">
                        <img src="images/stitching.png" alt="Stitching research visualization" />
                    </div>
                    <div class="research-content">
                        <papertitle>Closing the Gap between TD Learning and Supervised Learning &ndash; A Generalisation Point of View</papertitle> <em style="color:#696969;">[ICLR 2024]</em> <br> 
                        <authors>
                            <b>Raj Ghugare</b>,
                            Matthieu Geist,
                            Glen Berseth,
                            Benjamin Eysenbach
                        </authors>
                        <p>
                            <a href="https://arxiv.org/abs/2401.11237">paper</a> |
                            <a href="https://github.com/RajGhugare19/stitching-is-combinatorial-generalisation">code</a>
                        </p>
                        <hr style="height:1px;border:none;color:#e0e0e0;background-color:#e0e0e0;">
                        <abstract>
                            This paper explores the link between trajectory stitching and combinatorial generalization. Although stitching is mostly associated with dynamic programming, we show that significant progress (up to 10x) can be made using much simpler techniques.
                        </abstract>
                    </div>
                </article>

                <article class="research-item">
                    <div class="research-image">
                        <img src="images/chemrl.png" alt="AI generated molecule" />
                    </div>
                    <div class="research-content">
                        <papertitle>Searching for High-Value Molecules Using Reinforcement Learning and Transformers</papertitle> <em style="color:#696969;">[ICLR 2024]</em> <br> 
                        <authors>
                            <b>Raj Ghugare</b>,
                            Santiago Miret,
                            Adriana Hugessen,
                            Mariano Phielipp,
                            Glen Berseth
                        </authors>
                        <p>
                            <a href="https://chemrlformer.github.io/">project page</a> |
                            <a href="https://arxiv.org/abs/2310.02902">paper</a> |
                            <a href="https://github.com/montrealrobotics/RL4Chem">code</a>
                        </p>
                        <hr style="height:1px;border:none;color:#e0e0e0;background-color:#e0e0e0;">
                        <abstract>
                            Through extensive experiments spanning across datasets with 100 million molecules and 25+ reward functions, we uncover essential algorithmic choices for
                            efficient search with RL, and discover phenomena like reward hacking of protien docking scores.
                        </abstract>
                    </div>
                </article>

                <article class="research-item">
                    <div class="research-image">
                        <img src="images/alm.png" alt="Aligned objective molecule" />
                    </div>
                    <div class="research-content">
                        <papertitle>Simplifying Model-based RL: Learning Representations, Latent-space Models and Policies with One Objective</papertitle> <em style="color:#696969;">[ICLR 2023]</em> <br> 
                        <authors>
                            <b>Raj Ghugare</b>,
                            Homanga Bharadhwaj,
                            Benjamin Eysenbach,
                            Sergey Levine,
                            Ruslan Salakhutdinov
                        </authors>
                        <p>
                            <a href="https://alignedlatentmodels.github.io/">project page</a> |
                            <a href="https://arxiv.org/abs/2209.08466">paper</a> |
                            <a href="https://github.com/RajGhugare19/alm">code</a>
                        </p>
                        <hr style="height:1px;border:none;color:#e0e0e0;background-color:#e0e0e0;">
                        <abstract>
                            We present a joint objective for latent space model based RL which lower bounds the RL objective. 
                            Maximising this bound jointly with the encoder, model, and the policy boosts sample efficiency, without using 
                            techniques like ensembles of Q-networks and a high replay ratio.
                        </abstract>
                    </div>
                </article>
            </section>

            <section class="research">
                <h2>Mentoring</h2>
                <p>I have worked with the following students. If you'd like to collaborate, please drop me an <a href="mailto:rg9360@princeton.edu">email</a>!</p>
                
                <div class="minimal-block">
                    <ul class="minimal-list">
                        <li>
                            <a href="https://jinschofield.github.io/">Jin Schofield</a> (Undergraduate student at Princeton University, 2025) <br>
                            <a href="https://www.linkedin.com/in/ahmed-turkman/?originalSubdomain=za">Ahmed Turkman</a> (Google DeepMind Scholar and Msc - AI for Science at AIMS South Africa, 2025)
                        </li>
                    </ul>
                </div>
            </section>

            <section class="research">
                <h2>Teaching</h2>
                <div class="minimal-block">
                    <ul class="minimal-list">
                        <li>
                            Teaching Assistant - COS 597R Probabilistic Topics in Reinforcement Learning (Fall 2025) <br>
                            Teaching Assistant - COS 435 Introduction to Reinforcement Learning (Spring 2026)
                        </li>
                    </ul>
                </div>
            </section>

            <p style="text-align:center;font-size:12px;color:#696969">
                    Last updated: February 2026.
            </p>

        </main>

    </div>
</body>
</html>